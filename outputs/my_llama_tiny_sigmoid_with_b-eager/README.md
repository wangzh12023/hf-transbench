---
library_name: transformers
base_model: configs/my_llama_tiny_sigmoid_with_b.json
tags:
- generated_from_trainer
datasets:
- wikitext
metrics:
- accuracy
model-index:
- name: my_llama_tiny_sigmoid_with_b-eager
  results:
  - task:
      name: Causal Language Modeling
      type: text-generation
    dataset:
      name: wikitext wikitext-103-raw-v1
      type: wikitext
      args: wikitext-103-raw-v1
    metrics:
    - name: Accuracy
      type: accuracy
      value: 0.9718981555815948
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# my_llama_tiny_sigmoid_with_b-eager

This model is a fine-tuned version of [configs/my_llama_tiny_sigmoid_with_b.json](https://huggingface.co/configs/my_llama_tiny_sigmoid_with_b.json) on the wikitext wikitext-103-raw-v1 dataset.
It achieves the following results on the evaluation set:
- Loss: 0.1297
- Accuracy: 0.9719

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0003
- train_batch_size: 32
- eval_batch_size: 16
- seed: 42
- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.015
- num_epochs: 1.0

### Training results

| Training Loss | Epoch  | Step  | Accuracy | Validation Loss |
|:-------------:|:------:|:-----:|:--------:|:---------------:|
| 6.0453        | 0.0118 | 200   | 0.1870   | 5.8311          |
| 5.4013        | 0.0235 | 400   | 0.2066   | 5.3403          |
| 5.2565        | 0.0353 | 600   | 0.2092   | 5.1610          |
| 5.0898        | 0.0471 | 800   | 0.2158   | 5.0402          |
| 5.0275        | 0.0588 | 1000  | 0.2178   | 4.9623          |
| 4.9513        | 0.0706 | 1200  | 0.2183   | 4.9012          |
| 4.9335        | 0.0823 | 1400  | 0.2201   | 4.8573          |
| 4.9042        | 0.0941 | 1600  | 4.8225   | 0.2201          |
| 4.8306        | 0.1059 | 1800  | 4.7905   | 0.2230          |
| 4.8209        | 0.1176 | 2000  | 4.7487   | 0.2240          |
| 4.7983        | 0.1294 | 2200  | 4.7251   | 0.2244          |
| 4.7604        | 0.1412 | 2400  | 4.6985   | 0.2260          |
| 4.746         | 0.1529 | 2600  | 4.6746   | 0.2283          |
| 4.7025        | 0.1647 | 2800  | 4.6278   | 0.2308          |
| 4.6843        | 0.1764 | 3000  | 4.5871   | 0.2331          |
| 4.6371        | 0.1882 | 3200  | 4.5632   | 0.2349          |
| 4.6081        | 0.2000 | 3400  | 4.5167   | 0.2405          |
| 4.5468        | 0.2117 | 3600  | 4.4445   | 0.2475          |
| 4.3741        | 0.2235 | 3800  | 4.2687   | 0.2753          |
| 3.7477        | 0.2353 | 4000  | 3.5835   | 0.3854          |
| 3.0244        | 0.2470 | 4200  | 2.8896   | 0.5044          |
| 2.5325        | 0.2588 | 4400  | 2.4269   | 0.5889          |
| 2.1672        | 0.2705 | 4600  | 2.0571   | 0.6483          |
| 1.8841        | 0.2823 | 4800  | 1.8202   | 0.6813          |
| 1.7016        | 0.2941 | 5000  | 1.6280   | 0.7078          |
| 1.5357        | 0.3058 | 5200  | 1.5056   | 0.7229          |
| 1.4647        | 0.3176 | 5400  | 1.4158   | 0.7350          |
| 1.4121        | 0.3294 | 5600  | 1.3514   | 0.7426          |
| 1.3708        | 0.3411 | 5800  | 1.3010   | 0.7504          |
| 1.2811        | 0.3529 | 6000  | 1.2571   | 0.7569          |
| 1.2234        | 0.3646 | 6200  | 1.1985   | 0.7665          |
| 1.1547        | 0.3764 | 6400  | 1.1185   | 0.7805          |
| 0.9986        | 0.3882 | 6600  | 0.9623   | 0.8089          |
| 0.8315        | 0.3999 | 6800  | 0.7994   | 0.8414          |
| 0.7206        | 0.4117 | 7000  | 0.6830   | 0.8658          |
| 0.606         | 0.4235 | 7200  | 0.5939   | 0.8850          |
| 0.5599        | 0.4352 | 7400  | 0.5291   | 0.8989          |
| 0.485         | 0.4470 | 7600  | 0.4753   | 0.9096          |
| 0.4516        | 0.4587 | 7800  | 0.4309   | 0.9171          |
| 0.4061        | 0.4705 | 8000  | 0.3876   | 0.9244          |
| 0.3698        | 0.4823 | 8200  | 0.3510   | 0.9306          |
| 0.3392        | 0.4940 | 8400  | 0.3206   | 0.9360          |
| 0.3055        | 0.5058 | 8600  | 0.2909   | 0.9407          |
| 0.2824        | 0.5176 | 8800  | 0.2713   | 0.9447          |
| 0.2671        | 0.5293 | 9000  | 0.2555   | 0.9479          |
| 0.2498        | 0.5411 | 9200  | 0.2412   | 0.9504          |
| 0.2431        | 0.5528 | 9400  | 0.2339   | 0.9513          |
| 0.2263        | 0.5646 | 9600  | 0.2201   | 0.9541          |
| 0.2188        | 0.5764 | 9800  | 0.2106   | 0.9559          |
| 0.2136        | 0.5881 | 10000 | 0.2010   | 0.9577          |
| 0.2042        | 0.5999 | 10200 | 0.1946   | 0.9586          |
| 0.1979        | 0.6117 | 10400 | 0.1893   | 0.9600          |
| 0.1914        | 0.6234 | 10600 | 0.1844   | 0.9607          |
| 0.1881        | 0.6352 | 10800 | 0.1792   | 0.9620          |
| 0.181         | 0.6469 | 11000 | 0.1771   | 0.9621          |
| 0.1784        | 0.6587 | 11200 | 0.1724   | 0.9632          |
| 0.1827        | 0.6705 | 11400 | 0.1688   | 0.9638          |
| 0.1692        | 0.6822 | 11600 | 0.1667   | 0.9642          |
| 0.1745        | 0.6940 | 11800 | 0.1632   | 0.9649          |
| 0.1691        | 0.7058 | 12000 | 0.1602   | 0.9654          |
| 0.1665        | 0.7175 | 12200 | 0.1586   | 0.9659          |
| 0.1652        | 0.7293 | 12400 | 0.1557   | 0.9663          |
| 0.163         | 0.7410 | 12600 | 0.1540   | 0.9668          |
| 0.1547        | 0.7528 | 12800 | 0.1513   | 0.9675          |
| 0.1567        | 0.7646 | 13000 | 0.1492   | 0.9678          |
| 0.1524        | 0.7763 | 13200 | 0.1476   | 0.9682          |
| 0.1489        | 0.7881 | 13400 | 0.1444   | 0.9688          |
| 0.1476        | 0.7999 | 13600 | 0.1418   | 0.9693          |
| 0.1506        | 0.8116 | 13800 | 0.1406   | 0.9695          |
| 0.1446        | 0.8234 | 14000 | 0.1386   | 0.9701          |
| 0.1441        | 0.8351 | 14200 | 0.1376   | 0.9703          |
| 0.1415        | 0.8469 | 14400 | 0.1361   | 0.9706          |
| 0.1424        | 0.8587 | 14600 | 0.1350   | 0.9707          |
| 0.1396        | 0.8704 | 14800 | 0.1341   | 0.9709          |
| 0.1415        | 0.8822 | 15000 | 0.1330   | 0.9711          |
| 0.1397        | 0.8940 | 15200 | 0.1324   | 0.9714          |
| 0.1408        | 0.9057 | 15400 | 0.1318   | 0.9715          |
| 0.1362        | 0.9175 | 15600 | 0.1312   | 0.9716          |
| 0.1356        | 0.9292 | 15800 | 0.1307   | 0.9716          |
| 0.136         | 0.9410 | 16000 | 0.1303   | 0.9717          |
| 0.1403        | 0.9528 | 16200 | 0.1301   | 0.9718          |
| 0.1322        | 0.9645 | 16400 | 0.1299   | 0.9719          |
| 0.1348        | 0.9763 | 16600 | 0.1298   | 0.9719          |
| 0.1318        | 0.9881 | 16800 | 0.1297   | 0.9719          |
| 0.1359        | 0.9998 | 17000 | 0.1297   | 0.9719          |


### Framework versions

- Transformers 4.47.1
- Pytorch 2.5.1
- Datasets 3.5.0
- Tokenizers 0.21.1
