---
library_name: transformers
base_model: configs/my_llama_tiny_softmax.json
tags:
- generated_from_trainer
datasets:
- wikitext
metrics:
- accuracy
model-index:
- name: my_llama_tiny_my_softmax-eager
  results:
  - task:
      name: Causal Language Modeling
      type: text-generation
    dataset:
      name: wikitext wikitext-103-raw-v1
      type: wikitext
      args: wikitext-103-raw-v1
    metrics:
    - name: Accuracy
      type: accuracy
      value: 0.04707212670085167
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# my_llama_tiny_my_softmax-eager

This model is a fine-tuned version of [configs/my_llama_tiny_softmax.json](https://huggingface.co/configs/my_llama_tiny_softmax.json) on the wikitext wikitext-103-raw-v1 dataset.
It achieves the following results on the evaluation set:
- Loss: 6.9430
- Accuracy: 0.0471

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0003
- train_batch_size: 32
- eval_batch_size: 16
- seed: 42
- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.015
- num_epochs: 1.0

### Training results

| Training Loss | Epoch  | Step  | Validation Loss | Accuracy |
|:-------------:|:------:|:-----:|:---------------:|:--------:|
| 7.1383        | 0.0118 | 200   | 7.1199          | 0.0443   |
| 7.0557        | 0.0235 | 400   | 7.0420          | 0.0460   |
| 7.0543        | 0.0353 | 600   | 6.9967          | 0.0466   |
| 6.998         | 0.0471 | 800   | 6.9753          | 0.0428   |
| 6.985         | 0.0588 | 1000  | 6.9526          | 0.0471   |
| 6.9781        | 0.0706 | 1200  | 6.9480          | 0.0464   |
| 6.9807        | 0.0823 | 1400  | 6.9430          | 0.0471   |
| 6.9817        | 0.0941 | 1600  | 6.9618          | 0.0456   |
| 6.9408        | 0.1059 | 1800  | 6.9549          | 0.0458   |
| 7.0089        | 0.1176 | 2000  | 6.9647          | 0.0463   |
| 6.9963        | 0.1294 | 2200  | 6.9539          | 0.0466   |
| 6.9884        | 0.1412 | 2400  | 6.9505          | 0.0465   |
| 7.0004        | 0.1529 | 2600  | 6.9540          | 0.0458   |
| 6.9885        | 0.1647 | 2800  | 6.9478          | 0.0453   |
| 7.0089        | 0.1764 | 3000  | 6.9623          | 0.0446   |
| 7.0022        | 0.1882 | 3200  | 6.9502          | 0.0452   |
| 6.9799        | 0.2000 | 3400  | 6.9504          | 0.0450   |
| 7.0146        | 0.2117 | 3600  | 6.9514          | 0.0449   |
| 7.0074        | 0.2235 | 3800  | 6.9759          | 0.0443   |
| 6.9847        | 0.2353 | 4000  | 6.9478          | 0.0464   |
| 6.9841        | 0.2470 | 4200  | 6.9557          | 0.0468   |
| 6.9834        | 0.2588 | 4400  | 6.9591          | 0.0458   |
| 7.0188        | 0.2705 | 4600  | 6.9653          | 0.0460   |
| 7.0121        | 0.2823 | 4800  | 6.9742          | 0.0453   |
| 7.0395        | 0.2941 | 5000  | 6.9779          | 0.0447   |
| 6.9878        | 0.3058 | 5200  | 6.9690          | 0.0451   |
| 7.0147        | 0.3176 | 5400  | 6.9645          | 0.0454   |
| 7.0229        | 0.3294 | 5600  | 6.9542          | 0.0460   |
| 7.0029        | 0.3411 | 5800  | 6.9717          | 0.0442   |
| 7.002         | 0.3529 | 6000  | 6.9732          | 0.0437   |
| 6.9998        | 0.3646 | 6200  | 6.9814          | 0.0432   |
| 7.0095        | 0.3764 | 6400  | 6.9806          | 0.0432   |
| 7.0228        | 0.3882 | 6600  | 6.9853          | 0.0443   |
| 7.0253        | 0.3999 | 6800  | 6.9886          | 0.0446   |
| 7.0351        | 0.4117 | 7000  | 6.9853          | 0.0455   |
| 7.0           | 0.4235 | 7200  | 6.9794          | 0.0459   |
| 7.0492        | 0.4352 | 7400  | 6.9711          | 0.0458   |
| 6.9848        | 0.4470 | 7600  | 6.9715          | 0.0472   |
| 7.028         | 0.4587 | 7800  | 6.9814          | 0.0455   |
| 7.0227        | 0.4705 | 8000  | 6.9688          | 0.0468   |
| 7.018         | 0.4823 | 8200  | 6.9666          | 0.0461   |
| 7.0106        | 0.4940 | 8400  | 6.9642          | 0.0470   |
| 7.0166        | 0.5058 | 8600  | 6.9653          | 0.0462   |
| 6.9607        | 0.5176 | 8800  | 6.9744          | 0.0449   |
| 7.0156        | 0.5293 | 9000  | 6.9809          | 0.0450   |
| 6.9737        | 0.5411 | 9200  | 6.9793          | 0.0455   |
| 6.991         | 0.5528 | 9400  | 6.9552          | 0.0469   |
| 6.9988        | 0.5646 | 9600  | 6.9634          | 0.0460   |
| 7.0786        | 0.5764 | 9800  | 7.0522          | 0.0460   |
| 7.0665        | 0.5881 | 10000 | 7.0211          | 0.0451   |
| 7.0675        | 0.5999 | 10200 | 7.0405          | 0.0453   |
| 7.1032        | 0.6117 | 10400 | 7.0502          | 0.0453   |
| 7.0261        | 0.6234 | 10600 | 7.0114          | 0.0448   |
| 7.1922        | 0.6352 | 10800 | 7.0791          | 0.0433   |
| 7.7978        | 0.6469 | 11000 | 7.6810          | 0.0379   |
| 7.1217        | 0.6587 | 11200 | 7.0941          | 0.0419   |
| 7.1463        | 0.6705 | 11400 | 7.0688          | 0.0424   |
| 7.0854        | 0.6822 | 11600 | 7.0699          | 0.0425   |
| 7.1952        | 0.6940 | 11800 | 7.0724          | 0.0420   |
| 7.1323        | 0.7058 | 12000 | 7.0711          | 0.0424   |
| 7.1204        | 0.7175 | 12200 | 7.0952          | 0.0423   |
| 7.1555        | 0.7293 | 12400 | 7.0987          | 0.0425   |
| 7.1294        | 0.7410 | 12600 | 7.0982          | 0.0426   |
| 7.1278        | 0.7528 | 12800 | 7.1135          | 0.0420   |
| 7.3391        | 0.7646 | 13000 | 7.3231          | 0.0409   |
| 7.9572        | 0.7763 | 13200 | 8.0727          | 0.0323   |
| 9.1295        | 0.7881 | 13400 | 9.2187          | 0.0232   |
| 9.466         | 0.7999 | 13600 | 9.4506          | 0.0222   |
| 9.3831        | 0.8116 | 13800 | 9.3534          | 0.0225   |
| 9.2679        | 0.8234 | 14000 | 9.2409          | 0.0234   |
| 9.1907        | 0.8351 | 14200 | 9.1720          | 0.0237   |
| 9.119         | 0.8469 | 14400 | 9.1219          | 0.0242   |
| 9.1           | 0.8587 | 14600 | 9.0863          | 0.0239   |
| 9.0651        | 0.8704 | 14800 | 9.0364          | 0.0242   |
| 9.0083        | 0.8822 | 15000 | 9.0125          | 0.0244   |
| 8.9872        | 0.8940 | 15200 | 8.9821          | 0.0246   |
| 8.9692        | 0.9057 | 15400 | 8.9634          | 0.0248   |
| 8.9409        | 0.9175 | 15600 | 8.9442          | 0.0251   |
| 8.9501        | 0.9292 | 15800 | 8.9389          | 0.0251   |
| 8.9436        | 0.9410 | 16000 | 8.9165          | 0.0252   |
| 8.9253        | 0.9528 | 16200 | 8.9079          | 0.0252   |
| 8.9146        | 0.9645 | 16400 | 8.9124          | 0.0250   |
| 8.9241        | 0.9763 | 16600 | 8.9143          | 0.0247   |
| 8.9086        | 0.9881 | 16800 | 8.9151          | 0.0247   |
| 8.9255        | 0.9998 | 17000 | 8.9153          | 0.0247   |


### Framework versions

- Transformers 4.47.1
- Pytorch 2.5.1
- Datasets 3.5.0
- Tokenizers 0.21.1
