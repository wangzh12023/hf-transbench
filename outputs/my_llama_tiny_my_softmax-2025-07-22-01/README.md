---
library_name: transformers
base_model: configs/my_llama_tiny_softmax.json
tags:
- generated_from_trainer
datasets:
- wikitext
metrics:
- accuracy
model-index:
- name: my_llama_tiny_my_softmax-2025-07-22-01
  results:
  - task:
      name: Causal Language Modeling
      type: text-generation
    dataset:
      name: wikitext wikitext-103-raw-v1
      type: wikitext
      args: wikitext-103-raw-v1
    metrics:
    - name: Accuracy
      type: accuracy
      value: 0.05551019571537402
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# my_llama_tiny_my_softmax-2025-07-22-01

This model is a fine-tuned version of [configs/my_llama_tiny_softmax.json](https://huggingface.co/configs/my_llama_tiny_softmax.json) on the wikitext wikitext-103-raw-v1 dataset.
It achieves the following results on the evaluation set:
- Loss: 6.5697
- Accuracy: 0.0555

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0003
- train_batch_size: 32
- eval_batch_size: 16
- seed: 42
- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.015
- num_epochs: 1.0

### Training results

| Training Loss | Epoch  | Step  | Validation Loss | Accuracy |
|:-------------:|:------:|:-----:|:---------------:|:--------:|
| 7.1382        | 0.0118 | 200   | 7.0810          | 0.0441   |
| 7.0556        | 0.0235 | 400   | 7.0455          | 0.0450   |
| 7.0558        | 0.0353 | 600   | 6.9975          | 0.0463   |
| 6.9881        | 0.0471 | 800   | 6.9598          | 0.0450   |
| 6.9739        | 0.0588 | 1000  | 6.9350          | 0.0478   |
| 6.9408        | 0.0706 | 1200  | 6.9107          | 0.0492   |
| 6.939         | 0.0823 | 1400  | 6.8905          | 0.0492   |
| 6.9279        | 0.0941 | 1600  | 6.8832          | 0.0494   |
| 6.8691        | 0.1059 | 1800  | 6.8715          | 0.0488   |
| 6.914         | 0.1176 | 2000  | 6.8603          | 0.0497   |
| 6.9053        | 0.1294 | 2200  | 6.8519          | 0.0498   |
| 6.8876        | 0.1412 | 2400  | 6.8465          | 0.0486   |
| 6.8862        | 0.1529 | 2600  | 6.8338          | 0.0498   |
| 6.8656        | 0.1647 | 2800  | 6.8269          | 0.0503   |
| 6.8816        | 0.1764 | 3000  | 6.8069          | 0.0500   |
| 6.8793        | 0.1882 | 3200  | 6.8023          | 0.0504   |
| 6.842         | 0.2000 | 3400  | 6.8054          | 0.0499   |
| 6.872         | 0.2117 | 3600  | 6.7977          | 0.0507   |
| 6.8322        | 0.2235 | 3800  | 6.7832          | 0.0509   |
| 6.8248        | 0.2353 | 4000  | 6.7799          | 0.0509   |
| 6.8112        | 0.2470 | 4200  | 6.7737          | 0.0506   |
| 6.7857        | 0.2588 | 4400  | 6.7653          | 0.0515   |
| 6.8206        | 0.2705 | 4600  | 6.7539          | 0.0522   |
| 6.7997        | 0.2823 | 4800  | 6.7472          | 0.0521   |
| 6.8058        | 0.2941 | 5000  | 6.7382          | 0.0520   |
| 6.7433        | 0.3058 | 5200  | 6.7378          | 0.0518   |
| 6.7748        | 0.3176 | 5400  | 6.7279          | 0.0520   |
| 6.793         | 0.3294 | 5600  | 6.7204          | 0.0521   |
| 6.767         | 0.3411 | 5800  | 6.7152          | 0.0527   |
| 6.7375        | 0.3529 | 6000  | 6.7146          | 0.0530   |
| 6.7286        | 0.3646 | 6200  | 6.7067          | 0.0523   |
| 6.7261        | 0.3764 | 6400  | 6.7047          | 0.0523   |
| 6.7093        | 0.3882 | 6600  | 6.7062          | 0.0520   |
| 6.7369        | 0.3999 | 6800  | 6.6923          | 0.0530   |
| 6.7342        | 0.4117 | 7000  | 6.6895          | 0.0528   |
| 6.7044        | 0.4235 | 7200  | 6.6812          | 0.0533   |
| 6.763         | 0.4352 | 7400  | 6.6812          | 0.0533   |
| 6.6769        | 0.4470 | 7600  | 6.6768          | 0.0531   |
| 6.7346        | 0.4587 | 7800  | 6.6701          | 0.0531   |
| 6.7251        | 0.4705 | 8000  | 6.6658          | 0.0538   |
| 6.7041        | 0.4823 | 8200  | 6.6636          | 0.0540   |
| 6.7207        | 0.4940 | 8400  | 6.6544          | 0.0538   |
| 6.7053        | 0.5058 | 8600  | 6.6470          | 0.0541   |
| 6.6534        | 0.5176 | 8800  | 6.6424          | 0.0538   |
| 6.7075        | 0.5293 | 9000  | 6.6357          | 0.0538   |
| 6.6374        | 0.5411 | 9200  | 6.6354          | 0.0536   |
| 6.666         | 0.5528 | 9400  | 6.6322          | 0.0536   |
| 6.6741        | 0.5646 | 9600  | 6.6294          | 0.0543   |
| 6.6429        | 0.5764 | 9800  | 6.6285          | 0.0538   |
| 6.6703        | 0.5881 | 10000 | 6.6230          | 0.0545   |
| 6.6593        | 0.5999 | 10200 | 6.6212          | 0.0541   |
| 6.6461        | 0.6117 | 10400 | 6.6217          | 0.0543   |
| 6.6309        | 0.6234 | 10600 | 6.6149          | 0.0542   |
| 6.6371        | 0.6352 | 10800 | 6.6116          | 0.0544   |
| 6.6492        | 0.6469 | 11000 | 6.6076          | 0.0550   |
| 6.623         | 0.6587 | 11200 | 6.6045          | 0.0547   |
| 6.6915        | 0.6705 | 11400 | 6.6035          | 0.0549   |
| 6.6167        | 0.6822 | 11600 | 6.6009          | 0.0548   |
| 6.6529        | 0.6940 | 11800 | 6.5989          | 0.0547   |
| 6.6579        | 0.7058 | 12000 | 6.5981          | 0.0550   |
| 6.6218        | 0.7175 | 12200 | 6.5948          | 0.0552   |
| 6.6478        | 0.7293 | 12400 | 6.5919          | 0.0552   |
| 6.6347        | 0.7410 | 12600 | 6.5916          | 0.0552   |
| 6.6059        | 0.7528 | 12800 | 6.5881          | 0.0553   |
| 6.6233        | 0.7646 | 13000 | 6.5860          | 0.0553   |
| 6.6012        | 0.7763 | 13200 | 6.5835          | 0.0554   |
| 6.6211        | 0.7881 | 13400 | 6.5839          | 0.0552   |
| 6.6212        | 0.7999 | 13600 | 6.5812          | 0.0553   |
| 6.6222        | 0.8116 | 13800 | 6.5796          | 0.0553   |
| 6.6467        | 0.8234 | 14000 | 6.5782          | 0.0556   |
| 6.6133        | 0.8351 | 14200 | 6.5776          | 0.0557   |
| 6.6278        | 0.8469 | 14400 | 6.5760          | 0.0554   |
| 6.6399        | 0.8587 | 14600 | 6.5755          | 0.0554   |
| 6.6371        | 0.8704 | 14800 | 6.5738          | 0.0556   |
| 6.5941        | 0.8822 | 15000 | 6.5729          | 0.0555   |
| 6.6257        | 0.8940 | 15200 | 6.5722          | 0.0554   |
| 6.6233        | 0.9057 | 15400 | 6.5714          | 0.0557   |
| 6.6185        | 0.9175 | 15600 | 6.5709          | 0.0556   |
| 6.5971        | 0.9292 | 15800 | 6.5703          | 0.0556   |
| 6.6104        | 0.9410 | 16000 | 6.5703          | 0.0556   |
| 6.6543        | 0.9528 | 16200 | 6.5701          | 0.0556   |
| 6.5834        | 0.9645 | 16400 | 6.5698          | 0.0556   |
| 6.6161        | 0.9763 | 16600 | 6.5697          | 0.0555   |
| 6.5482        | 0.9881 | 16800 | 6.5697          | 0.0555   |
| 6.6005        | 0.9998 | 17000 | 6.5697          | 0.0555   |


### Framework versions

- Transformers 4.47.1
- Pytorch 2.5.1
- Datasets 3.5.0
- Tokenizers 0.21.1
