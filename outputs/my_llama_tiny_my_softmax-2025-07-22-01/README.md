---
library_name: transformers
base_model: configs/my_llama_tiny_softmax.json
tags:
- generated_from_trainer
datasets:
- wikitext
metrics:
- accuracy
model-index:
- name: my_llama_tiny_my_softmax-2025-07-22-01
  results:
  - task:
      name: Causal Language Modeling
      type: text-generation
    dataset:
      name: wikitext wikitext-103-raw-v1
      type: wikitext
      args: wikitext-103-raw-v1
    metrics:
    - name: Accuracy
      type: accuracy
      value: 0.039525691699604744
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# my_llama_tiny_my_softmax-2025-07-22-01

This model is a fine-tuned version of [configs/my_llama_tiny_softmax.json](https://huggingface.co/configs/my_llama_tiny_softmax.json) on the wikitext wikitext-103-raw-v1 dataset.
It achieves the following results on the evaluation set:
- Loss: 7.0957
- Accuracy: 0.0395

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0003
- train_batch_size: 32
- eval_batch_size: 16
- seed: 42
- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.015
- num_epochs: 1.0

### Training results

| Training Loss | Epoch  | Step  | Accuracy | Validation Loss |
|:-------------:|:------:|:-----:|:--------:|:---------------:|
| 7.1382        | 0.0118 | 200   | 0.0441   | 7.0810          |
| 7.0556        | 0.0235 | 400   | 0.0450   | 7.0455          |
| 7.0558        | 0.0353 | 600   | 0.0463   | 6.9975          |
| 6.9881        | 0.0471 | 800   | 0.0450   | 6.9598          |
| 6.9739        | 0.0588 | 1000  | 0.0478   | 6.9350          |
| 6.9408        | 0.0706 | 1200  | 0.0492   | 6.9107          |
| 6.939         | 0.0823 | 1400  | 0.0492   | 6.8905          |
| 6.9279        | 0.0941 | 1600  | 0.0494   | 6.8832          |
| 6.8691        | 0.1059 | 1800  | 0.0488   | 6.8715          |
| 6.914         | 0.1176 | 2000  | 0.0497   | 6.8603          |
| 6.9053        | 0.1294 | 2200  | 0.0498   | 6.8519          |
| 6.8876        | 0.1412 | 2400  | 0.0486   | 6.8465          |
| 6.8862        | 0.1529 | 2600  | 0.0498   | 6.8338          |
| 6.8656        | 0.1647 | 2800  | 0.0503   | 6.8269          |
| 6.8816        | 0.1764 | 3000  | 0.0500   | 6.8069          |
| 6.8793        | 0.1882 | 3200  | 0.0504   | 6.8023          |
| 6.842         | 0.2000 | 3400  | 0.0499   | 6.8054          |
| 6.872         | 0.2117 | 3600  | 0.0507   | 6.7977          |
| 6.8322        | 0.2235 | 3800  | 0.0509   | 6.7832          |
| 6.8248        | 0.2353 | 4000  | 0.0509   | 6.7799          |
| 6.8112        | 0.2470 | 4200  | 0.0506   | 6.7737          |
| 6.7857        | 0.2588 | 4400  | 0.0515   | 6.7653          |
| 6.8206        | 0.2705 | 4600  | 0.0522   | 6.7539          |
| 6.7997        | 0.2823 | 4800  | 0.0521   | 6.7472          |
| 6.8058        | 0.2941 | 5000  | 0.0520   | 6.7382          |
| 6.7433        | 0.3058 | 5200  | 0.0518   | 6.7378          |
| 6.7748        | 0.3176 | 5400  | 0.0520   | 6.7279          |
| 6.793         | 0.3294 | 5600  | 0.0521   | 6.7204          |
| 6.767         | 0.3411 | 5800  | 0.0527   | 6.7152          |
| 6.7375        | 0.3529 | 6000  | 0.0530   | 6.7146          |
| 6.7286        | 0.3646 | 6200  | 0.0523   | 6.7067          |
| 6.7261        | 0.3764 | 6400  | 0.0523   | 6.7047          |
| 6.7093        | 0.3882 | 6600  | 0.0520   | 6.7062          |
| 6.7369        | 0.3999 | 6800  | 0.0530   | 6.6923          |
| 6.7342        | 0.4117 | 7000  | 0.0528   | 6.6895          |
| 6.7044        | 0.4235 | 7200  | 0.0533   | 6.6812          |
| 6.763         | 0.4352 | 7400  | 0.0533   | 6.6812          |
| 6.6769        | 0.4470 | 7600  | 0.0531   | 6.6768          |
| 6.7346        | 0.4587 | 7800  | 0.0531   | 6.6701          |
| 6.7251        | 0.4705 | 8000  | 0.0538   | 6.6658          |
| 6.7041        | 0.4823 | 8200  | 0.0540   | 6.6636          |
| 6.7207        | 0.4940 | 8400  | 0.0538   | 6.6544          |
| 6.7053        | 0.5058 | 8600  | 0.0541   | 6.6470          |
| 6.6534        | 0.5176 | 8800  | 0.0538   | 6.6424          |
| 6.7075        | 0.5293 | 9000  | 0.0538   | 6.6357          |
| 6.6374        | 0.5411 | 9200  | 0.0536   | 6.6354          |
| 6.666         | 0.5528 | 9400  | 0.0536   | 6.6322          |
| 6.6741        | 0.5646 | 9600  | 0.0543   | 6.6294          |
| 6.6429        | 0.5764 | 9800  | 0.0538   | 6.6285          |
| 6.6703        | 0.5881 | 10000 | 0.0545   | 6.6230          |
| 6.6593        | 0.5999 | 10200 | 0.0541   | 6.6212          |
| 6.6461        | 0.6117 | 10400 | 0.0543   | 6.6217          |
| 6.6309        | 0.6234 | 10600 | 0.0542   | 6.6149          |
| 6.6371        | 0.6352 | 10800 | 0.0544   | 6.6116          |
| 6.6492        | 0.6469 | 11000 | 0.0550   | 6.6076          |
| 6.623         | 0.6587 | 11200 | 0.0547   | 6.6045          |
| 6.6915        | 0.6705 | 11400 | 0.0549   | 6.6035          |
| 6.6167        | 0.6822 | 11600 | 0.0548   | 6.6009          |
| 6.6529        | 0.6940 | 11800 | 0.0547   | 6.5989          |
| 6.6579        | 0.7058 | 12000 | 0.0550   | 6.5981          |
| 6.6218        | 0.7175 | 12200 | 0.0552   | 6.5948          |
| 6.6478        | 0.7293 | 12400 | 0.0552   | 6.5919          |
| 6.6347        | 0.7410 | 12600 | 0.0552   | 6.5916          |
| 6.6059        | 0.7528 | 12800 | 0.0553   | 6.5881          |
| 6.6233        | 0.7646 | 13000 | 0.0553   | 6.5860          |
| 6.6012        | 0.7763 | 13200 | 0.0554   | 6.5835          |
| 6.6211        | 0.7881 | 13400 | 0.0552   | 6.5839          |
| 6.6212        | 0.7999 | 13600 | 0.0553   | 6.5812          |
| 6.6222        | 0.8116 | 13800 | 0.0553   | 6.5796          |
| 6.6467        | 0.8234 | 14000 | 0.0556   | 6.5782          |
| 6.6133        | 0.8351 | 14200 | 0.0557   | 6.5776          |
| 6.6278        | 0.8469 | 14400 | 0.0554   | 6.5760          |
| 6.6399        | 0.8587 | 14600 | 0.0554   | 6.5755          |
| 6.6371        | 0.8704 | 14800 | 0.0556   | 6.5738          |
| 6.5941        | 0.8822 | 15000 | 0.0555   | 6.5729          |
| 6.6257        | 0.8940 | 15200 | 0.0554   | 6.5722          |
| 6.6233        | 0.9057 | 15400 | 0.0557   | 6.5714          |
| 6.6185        | 0.9175 | 15600 | 0.0556   | 6.5709          |
| 6.5971        | 0.9292 | 15800 | 0.0556   | 6.5703          |
| 6.6104        | 0.9410 | 16000 | 0.0556   | 6.5703          |
| 6.6543        | 0.9528 | 16200 | 0.0556   | 6.5701          |
| 6.5834        | 0.9645 | 16400 | 0.0556   | 6.5698          |
| 6.6161        | 0.9763 | 16600 | 0.0555   | 6.5697          |
| 6.5482        | 0.9881 | 16800 | 0.0555   | 6.5697          |
| 6.6005        | 0.9998 | 17000 | 0.0555   | 6.5697          |


### Framework versions

- Transformers 4.47.1
- Pytorch 2.5.1
- Datasets 3.5.0
- Tokenizers 0.21.1
