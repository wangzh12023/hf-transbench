---
library_name: transformers
base_model: configs/my_llama_tiny_sigmoid.json
tags:
- generated_from_trainer
datasets:
- wikitext
metrics:
- accuracy
model-index:
- name: my_llama_tiny_sigmoid-eager
  results:
  - task:
      name: Causal Language Modeling
      type: text-generation
    dataset:
      name: wikitext wikitext-103-raw-v1
      type: wikitext
      args: wikitext-103-raw-v1
    metrics:
    - name: Accuracy
      type: accuracy
      value: 0.05852330376023586
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# my_llama_tiny_sigmoid-eager

This model is a fine-tuned version of [configs/my_llama_tiny_sigmoid.json](https://huggingface.co/configs/my_llama_tiny_sigmoid.json) on the wikitext wikitext-103-raw-v1 dataset.
It achieves the following results on the evaluation set:
- Loss: 6.4957
- Accuracy: 0.0585

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0003
- train_batch_size: 32
- eval_batch_size: 16
- seed: 42
- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.015
- num_epochs: 1.0

### Training results

| Training Loss | Epoch  | Step  | Accuracy | Validation Loss |
|:-------------:|:------:|:-----:|:--------:|:---------------:|
| 7.1379        | 0.0118 | 200   | 0.0465   | 7.0800          |
| 7.0618        | 0.0235 | 400   | 0.0453   | 7.0450          |
| 7.068         | 0.0353 | 600   | 0.0458   | 7.0061          |
| 7.001         | 0.0471 | 800   | 0.0458   | 6.9672          |
| 6.9849        | 0.0588 | 1000  | 0.0476   | 6.9463          |
| 6.9595        | 0.0706 | 1200  | 0.0492   | 6.9254          |
| 6.9615        | 0.0823 | 1400  | 0.0492   | 6.9143          |
| 6.9481        | 0.0941 | 1600  | 0.0492   | 6.9076          |
| 6.8933        | 0.1059 | 1800  | 0.0491   | 6.8981          |
| 6.9375        | 0.1176 | 2000  | 0.0487   | 6.8868          |
| 6.9232        | 0.1294 | 2200  | 0.0496   | 6.8763          |
| 6.913         | 0.1412 | 2400  | 0.0495   | 6.8726          |
| 6.9114        | 0.1529 | 2600  | 6.8608   | 0.0501          |
| 6.8837        | 0.1647 | 2800  | 6.8619   | 0.0502          |
| 6.902         | 0.1764 | 3000  | 6.8349   | 0.0509          |
| 6.8917        | 0.1882 | 3200  | 6.8224   | 0.0512          |
| 6.856         | 0.2000 | 3400  | 6.8176   | 0.0509          |
| 6.8822        | 0.2117 | 3600  | 6.8086   | 0.0508          |
| 6.8419        | 0.2235 | 3800  | 6.7971   | 0.0501          |
| 6.8288        | 0.2353 | 4000  | 6.7815   | 0.0515          |
| 6.8153        | 0.2470 | 4200  | 6.7849   | 0.0504          |
| 6.7904        | 0.2588 | 4400  | 6.7692   | 0.0508          |
| 6.8247        | 0.2705 | 4600  | 6.7620   | 0.0512          |
| 6.8025        | 0.2823 | 4800  | 6.7530   | 0.0513          |
| 6.8116        | 0.2941 | 5000  | 6.7464   | 0.0513          |
| 6.7469        | 0.3058 | 5200  | 6.7372   | 0.0513          |
| 6.7773        | 0.3176 | 5400  | 6.7298   | 0.0517          |
| 6.7938        | 0.3294 | 5600  | 6.7178   | 0.0518          |
| 6.7628        | 0.3411 | 5800  | 6.7128   | 0.0521          |
| 6.7294        | 0.3529 | 6000  | 6.7074   | 0.0522          |
| 6.7206        | 0.3646 | 6200  | 6.7005   | 0.0529          |
| 6.7148        | 0.3764 | 6400  | 6.6945   | 0.0521          |
| 6.6963        | 0.3882 | 6600  | 6.6914   | 0.0528          |
| 6.7225        | 0.3999 | 6800  | 6.6804   | 0.0531          |
| 6.7193        | 0.4117 | 7000  | 6.6705   | 0.0529          |
| 6.6881        | 0.4235 | 7200  | 6.6611   | 0.0531          |
| 6.7437        | 0.4352 | 7400  | 6.6590   | 0.0535          |
| 6.6494        | 0.4470 | 7600  | 6.6542   | 0.0531          |
| 6.7089        | 0.4587 | 7800  | 6.6446   | 0.0536          |
| 6.6884        | 0.4705 | 8000  | 6.6361   | 0.0536          |
| 6.6735        | 0.4823 | 8200  | 6.6375   | 0.0539          |
| 6.688         | 0.4940 | 8400  | 6.6238   | 0.0543          |
| 6.6731        | 0.5058 | 8600  | 6.6157   | 0.0543          |
| 6.6281        | 0.5176 | 8800  | 6.6127   | 0.0542          |
| 6.6693        | 0.5293 | 9000  | 6.6097   | 0.0547          |
| 6.6005        | 0.5411 | 9200  | 6.6022   | 0.0549          |
| 6.6314        | 0.5528 | 9400  | 6.6020   | 0.0548          |
| 6.6368        | 0.5646 | 9600  | 6.5981   | 0.0556          |
| 6.6057        | 0.5764 | 9800  | 6.5919   | 0.0554          |
| 6.6239        | 0.5881 | 10000 | 6.5800   | 0.0558          |
| 6.6165        | 0.5999 | 10200 | 6.5757   | 0.0559          |
| 6.5991        | 0.6117 | 10400 | 6.5811   | 0.0551          |
| 6.5863        | 0.6234 | 10600 | 6.5675   | 0.0560          |
| 6.5843        | 0.6352 | 10800 | 6.5652   | 0.0562          |
| 6.601         | 0.6469 | 11000 | 6.5599   | 0.0562          |
| 6.5718        | 0.6587 | 11200 | 6.5533   | 0.0564          |
| 6.6377        | 0.6705 | 11400 | 6.5517   | 0.0565          |
| 6.5641        | 0.6822 | 11600 | 6.5463   | 0.0570          |
| 6.5949        | 0.6940 | 11800 | 6.5469   | 0.0567          |
| 6.595         | 0.7058 | 12000 | 6.5378   | 0.0572          |
| 6.5642        | 0.7175 | 12200 | 6.5362   | 0.0572          |
| 6.5902        | 0.7293 | 12400 | 6.5330   | 0.0573          |
| 6.5721        | 0.7410 | 12600 | 6.5293   | 0.0575          |
| 6.542         | 0.7528 | 12800 | 6.5221   | 0.0578          |
| 6.5637        | 0.7646 | 13000 | 6.5206   | 0.0577          |
| 6.5382        | 0.7763 | 13200 | 6.5174   | 0.0577          |
| 6.5564        | 0.7881 | 13400 | 6.5165   | 0.0577          |
| 6.5558        | 0.7999 | 13600 | 6.5151   | 0.0580          |
| 6.5535        | 0.8116 | 13800 | 6.5101   | 0.0578          |
| 6.5808        | 0.8234 | 14000 | 6.5152   | 0.0576          |
| 6.5433        | 0.8351 | 14200 | 6.5073   | 0.0582          |
| 6.5559        | 0.8469 | 14400 | 6.5071   | 0.0582          |
| 6.5689        | 0.8587 | 14600 | 6.5045   | 0.0582          |
| 6.5693        | 0.8704 | 14800 | 6.5030   | 0.0582          |
| 6.5268        | 0.8822 | 15000 | 6.5008   | 0.0584          |
| 6.5525        | 0.8940 | 15200 | 6.4996   | 0.0584          |
| 6.5519        | 0.9057 | 15400 | 6.4984   | 0.0586          |
| 6.5458        | 0.9175 | 15600 | 6.4973   | 0.0584          |
| 6.528         | 0.9292 | 15800 | 6.4965   | 0.0584          |
| 6.5356        | 0.9410 | 16000 | 6.4964   | 0.0586          |
| 6.5825        | 0.9528 | 16200 | 6.4961   | 0.0585          |
| 6.5099        | 0.9645 | 16400 | 6.4960   | 0.0584          |
| 6.5422        | 0.9763 | 16600 | 6.4957   | 0.0585          |
| 6.4757        | 0.9881 | 16800 | 6.4957   | 0.0585          |
| 6.5277        | 0.9998 | 17000 | 6.4957   | 0.0585          |


### Framework versions

- Transformers 4.47.1
- Pytorch 2.5.1
- Datasets 3.5.0
- Tokenizers 0.21.1
